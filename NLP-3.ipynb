{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-11-10 11:08:27.270982: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-10 11:08:27.398122: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-11-10 11:08:28.183144: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-11-10 11:08:28.183221: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-11-10 11:08:28.183228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699614509858
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699614516504
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load IMDb reviews dataset\n",
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
        "\n",
        "# Convert TensorFlow Datasets tensors to NumPy arrays\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699614745244
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to your data\n",
        "train_examples = [clean_text(text.decode('utf-8')) for text in train_examples]\n",
        "test_examples = [clean_text(text.decode('utf-8')) for text in test_examples]\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699614750908
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_examples)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_examples)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_examples)\n",
        "\n",
        "# Padding\n",
        "max_len = max(len(seq) for seq in train_sequences)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Convert sequences to strings\n",
        "train_strings = [' '.join(map(str, seq)) for seq in train_padded]\n",
        "test_strings = [' '.join(map(str, seq)) for seq in test_padded]\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699615315852
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_strings, val_strings, train_labels, val_labels = train_test_split(train_strings, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Universal Sentence Encoder embedding layer\n",
        "embed_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699615333474
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the sentiment analysis model with additional LSTM layers\n",
        "model = models.Sequential([\n",
        "    embed_layer,\n",
        "    layers.Reshape((1, -1)),  # Reshape the 2D tensor to 3D tensor\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "    layers.Bidirectional(layers.LSTM(64)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699615487533
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert strings to NumPy arrays\n",
        "train_strings_np = np.array(train_strings)\n",
        "val_strings_np = np.array(val_strings)\n",
        "\n",
        "# Convert labels to NumPy arrays\n",
        "train_labels_np = np.array(train_labels)\n",
        "val_labels_np = np.array(val_labels)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_strings_np, train_labels_np, epochs=5, batch_size=32, validation_data=(val_strings_np, val_labels_np))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/10\n625/625 [==============================] - 86s 126ms/step - loss: 0.6936 - accuracy: 0.4998 - val_loss: 0.6939 - val_accuracy: 0.4848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6936 - accuracy: 0.4985 - val_loss: 0.6928 - val_accuracy: 0.5152\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6937 - val_accuracy: 0.4848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6931 - accuracy: 0.5099 - val_loss: 0.6934 - val_accuracy: 0.4848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6931 - accuracy: 0.5085 - val_loss: 0.6923 - val_accuracy: 0.5358\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6920 - accuracy: 0.5199 - val_loss: 0.6912 - val_accuracy: 0.5242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/10\n625/625 [==============================] - 78s 124ms/step - loss: 0.6888 - accuracy: 0.5370 - val_loss: 0.6861 - val_accuracy: 0.5504\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6860 - accuracy: 0.5508 - val_loss: 0.6822 - val_accuracy: 0.5588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6812 - accuracy: 0.5698 - val_loss: 0.6767 - val_accuracy: 0.5772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/10\n625/625 [==============================] - 77s 124ms/step - loss: 0.6821 - accuracy: 0.5645 - val_loss: 0.6761 - val_accuracy: 0.5734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fa7b1bcc460>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699616395786
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_examples, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "782/782 [==============================] - 20s 25ms/step - loss: 0.3226 - accuracy: 0.8582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nTest accuracy: 0.8582000136375427\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699613350606
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# Load IMDb reviews dataset\n",
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)\n",
        "\n",
        "# Convert strings to NumPy arrays\n",
        "train_strings = np.array(train_examples)\n",
        "test_strings = np.array(test_examples)\n",
        "\n",
        "# Convert labels to NumPy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Load Universal Sentence Encoder from TensorFlow Hub\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "embed_layer = hub.KerasLayer(module_url, input_shape=[], dtype=tf.string, trainable=True)\n",
        "\n",
        "\n",
        "\n",
        "# Build the sentiment analysis model with additional LSTM layers\n",
        "model = models.Sequential([\n",
        "    embed_layer,\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_strings, train_labels, epochs=1, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_strings, test_labels)\n",
        "print(f\"Test accuracy: {test_accuracy}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "704/704 [==============================] - 958s 1s/step - loss: 0.3121 - accuracy: 0.8680 - val_loss: 0.2369 - val_accuracy: 0.9036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 51/782 [>.............................] - ETA: 18s - loss: 0.2632 - accuracy: 0.8989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 54/782 [=>............................] - ETA: 18s - loss: 0.2596 - accuracy: 0.9010"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699625453155
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_examples, test_labels)\n",
        "print(f\"Test accuracy: {test_acc}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "782/782 [==============================] - 55s 70ms/step - loss: 0.2772 - accuracy: 0.8804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nTest accuracy: 0.8804000020027161\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699625556832
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "fr"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}